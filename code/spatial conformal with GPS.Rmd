---
title: "sp conformal with gps"
author: "mira"
date: "2022/6/12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(ggplot2)
library(igraph)
library(viridis)
library(grf)
library(gbm)
library(randomForest)
library(bartMachine)
library(xgboost)
library(nnet)
library(blockCV)
#library(MASS)
```

# Intro
Why spatial interference.

Why ITE in favor of ATE CATE CQTE: Better account for treatment effect heterogeneity (which could result from spatial process).

Why intervals instead of point estimate: Uncertainty quantification for sensitive decision making. Better coverage.

Why conformal prediction: Non-parametric. Flexible. Accommodates various prediction models.

How to handle non-exchangeability in spatial conformal prediction: Assume ‘local’ exchangeability, using distance kernel to restrict scope of training data points. Or use global spatial CV technique.

How to handle spatial interference: In the case of non-trivial spatial interference, treatment effect defined as total effect, direct and spillover effect. For estimation, introduce generalised propensity score (gps), which defines treatment probability as a function of ego and neighbouring units’ covariates. The gps restores the ignorability and SUTVA assumptions underpinning classic PO framework. Besides individual level covariates, network and neighbourhood properties (such as centrality metrics or local averages) can also be included (Forastiere et.al., 2021). Network uncertainty can also be incorporated (may or may not have time for this in this project).

Relation to prior work: Extension and synthesis of prior work to spatial interference (Forastiere), bringing together conformal counterfactual prediction (Barber, Candes, Lei) and gps (Hirano & Imbens) based techniques.



# The proposed routine of analysis

## Genralised propensity scores for spatial interference

### Interference and definitions of causal effects (Aronow; Forastiere; ...)
With interference, there can be multiple levels of exposure to unit $i$ even for binary treatments. That is, the actual exposure level is a function of the unit's treatment $Z_{i}$ and others' treatment program $G_{i}$. The generalised propensity score $e(x,g)$ (Hirano & Imbens, 2004) has been developed for these scenarios (Forastiere et.al., 2021). Exposure level can be treated as either continuous or discretised. Here, for simplicity, we start with four levels of treatment,  $(Z_{i}=0,G_{i}=0)$ , $(Z_{i}=0,G_{i}=1)$ , $(Z_{i}=1,G_{i}=0)$ , $(Z_{i}=1,G_{i}=1)$. Treatment $G_{i}$ is defined as a function of the proportion of unit i's neighbours getting treatment.

Alternatively, we consider Z to be a binary treatment, while G is a continuous neighborhood exposure, represented by a function of the proportion of a unit's neighbors getting treatment. In this case, conditional on Z, we can estimate a dose response function (DRF) reflecting how the potential outcome is affected by exposure to varying levels of neighborhood treatment. This is the spillover effect conditional on Z(x). The direct effect can be obtained by integrating over the empirical distribution of $X^g$ and take the difference $E[Y(1)]-E[Y(0)]$. The total effect can be further obtained as $E[E[Y(1)]-E[Y(0)]]$ .

In a more general setting, we can consider continuous treatment for Z. In this case, the GPS can be a bivariate Normal distribution (williams, 2020). And the DRF will be a response surface.

### The joint propensity score and its properties (Forastiere; Williams; ...)
The bivariate treatment to a unit and its neighborhood is a joint treatment. The joint propensity score is the probability of assignment to a particular individual and neighborhood treatment given the observed covariates. The covariates to predict the propensity score can include individual level attributes of unit $i$, as well as neighborhood level aggregate attributes and spatial network structural properties.  

The joint score can be factorised: $e(z,g,x)=P(Z_{i}=z,G_{i}=g|X_{i}=x)=P(Z_{i}=z|G_{i}=g,X_{i}^z=x^z)P(G_{i}=g|X_{i}^g=x^g)$ The factorised form is easier to work with, especially if we assume $Z_{i}$  is conditional independent of $G_{-i}$, that is, individual treatment assignments are determined by covariates indepedently. Unconfoundedness holds not only conditional on the joint propensity score but also conditional on the individual and neighborhood propensity scores. This gives an unbiased estimator for the average dose response function with both individual and neighbor propensity scores being adjusted for $E[E[Y_{i}|Z_{i}=z,G_{i}=g,\phi(X_{i}^z),\lambda(g,z,X_{i}^g)]|Z_{i}=z,G_{i}=g]$. The individual ps can be obtained with standard binary treatment ps methods. The neighborhood ps requires generalised ps methods, since $G_{i}$ is not binary. Still, the question remains, how do we define the influencing 'neighborhood' of a unit? In fact, is it possible at all to define it without referencing the observed outcomes (i.e. exogenously)???


### Estimate gps for binary individual treatment with spatial interference (new)
ref:  
https://github.com/cran/CausalGPS/blob/master/R/estimate_gps.R
https://github.com/cran/twangRDC/blob/master/R/ps.xgb.r
https://github.com/williazo/mvGPS/blob/master/R/mvGPS.R

```{r get_gps_version1, include=TRUE, eval=FALSE}
# binary treatment, 4 exposure levels 00, 01, 10, 11
# treatment probability ~ multinom or binom

# input
# tr          col vector of individual level binary treatments, numeric
# covar       dataframe/matrix of covariates
# A           spatial network A_ji, in the form of adjacency matrix or distance matrix 
# Atype       type of A, "dist" or "adj", 
#             A does not have to be symmetric. If directed, j in row index
# pstype      "joint" f(x)=P(Z=z,G=g|X=x), 
#             "conditional_g" f(x,z)=P(G=g|Z=z,X=x), 
#             "conditional_z" f(x,g)=P(Z=z|G=g,X=x),
# ps_pred_model  "xgboost" or "multinom" or "binomial" 
# output
# ps          if pstype=="joint", outputs joint ps(Z,G) and marginals Z, G
#             if pstype=="conditional_g", outputs marginal ps for G. 
#             if pstype=="conditional_z", outputs marginal ps for Z.

get_gps1 <- function(tr,
                    covar,
                    A,
                    Atype="dist",
                    pstype="joint",
                    ps_pred_model="binomial"){
  # check args insert code here
  
  # format treatment and covariates
  n<-length(tr)
  if (Atype=="dist") {
    # define treatment levels tl based on tr and A, 4 treatment levels
    # to do: these lines need to be put into a function  get_trnei1(tr,A,Atype)
    trnei <-get_trnei1(tr,A,Atype)
    tl <- paste0("t",tr,trnei) # treatment level, multinomial.
    # construct neighborhood covariates
    k <- ncol(covar) 
    covnei <- list()
     for (i in 1:k) {
      covnei[[i]] <- rowSums(matrix(covar[,i],nrow=n,ncol=n,byrow=TRUE)*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
  } else {
    Atype=="adj"
    # define treatment levels tl
    trnei <-get_trnei1(tr,A,Atype)
    tl <- paste0("t",tr,trnei)
    # construct neighborhood covariates
    k <- ncol(covar)
    covnei <- list()
    for (i in 1:k) {
      covnei[[i]] <- rowSums(matrix(covar[,i],nrow=n,ncol=n,byrow=TRUE)*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
    netA <- igraph::graph_from_adjacency_matrix(A,mode="undirected",weighted=NULL)
    dgr <- rowSums(A) # the node degrees
    btwnss <- betweenness(netA, weights = NA) # betweeness centrality
    covnei <- cbind.data.frame(covnei,dgr,btwnss)
  }
  covar0 <- cbind.data.frame(covar,covnei)
  # fit treatment model and estimate propensity
  if (pstype=="joint"){
    if (ps_pred_model=="multinom") {
         data0=cbind.data.frame(covar0,tl)
         pred0 <- nnet::multinom(tl~.,data0) 
         ps <- predict(pred0,newdata=data0,type="probs")# probs/ class
         # cols are ordered 00, 01, 10, 11
         ps <-ps[,c(4,3,2,1)]
         colnames(ps)<-c("t11","t10","t01","t00")
    } else {
         # "binomial"
         T_1 <- glm.fit(x=covar,y=tr,family=binomial())
         T_2 <- glm.fit(x=covnei,y=trnei,family=binomial())
         ps1 <-  T_1$fitted.values# p(Z|X_z)
         ps2 <-  T_2$fitted.values # p(G|X_g)
         gps1 <- ps1*ps2 # z=1,g=1
         gps2 <- ps1-gps1 # z=1,g=0 
         gps3 <- (1-ps1)*ps2 # z=0,g=1 
         gps4 <- 1-ps1-gps3 # z=0,g=0 
         ps<-cbind(gps1,gps2,gps3,gps4,ps1,ps2)
         colnames(ps)<-c("t11","t10","t01","t00","z","g")
    } 
  } else if (pstype=="conditional_g"){
    # P(G=g|Z=z,X=x)
    if (ps_pred_model=="xgboost"){
         data0=cbind.data.frame(covar0,tr,trnei)
         pred0 <- xgboost::xgboost(data=data0,label=trnei) 
         ps <- as.matrix(predict(pred0,newdata=data0))
    }else{
         pred0 <- glm.fit(x=c(tr,covar0),y=trnei,family=binomial())
         ps <- pred0$fitted.values
    }    
  } else {
    # "conditional_z" P(Z=z|G=g,X=x)
    if (ps_pred_model=="xgboost"){
         data0=cbind.data.frame(covar0,tr,trnei)
         pred0 <- xgboost::xgboost(data=data0,label=tr) 
         ps <- as.matrix(predict(pred0,newdata=data0))
    }else{
         pred0 <- glm.fit(x=c(trnei,covar0),y=tr,family=binomial())
         ps <- pred0$fitted.values
    }    
  }  
  return(ps)
}
```


```{r get_gps_version2,include=TRUE, eval=FALSE}
# binary treatment, binary? (continuous) and continuous exposure levels
# treatment probability ~ N2, marginal treatment prob ~ N

# this version is based on Z, G being considered bivarate exposure to unit i
# gps estimation assumes bivariate Normal distribution for the joint density 
# and Normal distribution for the marginal density of Z and G.
# In our case, Z is binary G can be continuous, modification is needed.
# the following is for Z and G as two continuous exposures

# input
# tr          col vector of individual level binary treatments, numeric
# covar       dataframe/matrix of covariates
# A           spatial network A_ji, in the form of adjacency matrix or distance matrix 
# Atype       type of A, "dist" or "adj", 
#             A does not have to be symmetric. If directed, j in row index
# pstype      / 
# pred_model  "lm" other options...?
# ps          return list of joint and marginal ps

get_gps2 <- function(tr,
                    covar,
                    A,
                    Atype="dist",
                    pred_model="lm"){
  # check args insert code here
  
  # format treatment and covariates
  n<-length(tr)
  if (Atype=="dist") {
    # define treatment levels tl based on tr and A, stratified to 4 treatment levels
    A0 <- A
    diag(A0) <-1
    A0 <- A^(-1)
    diag(A0) <- 0
    A0 <- A0/rowSums(A0) # row std inverse dist, can try other dist decay
    trnei <- rowSums(matrix(tr,nrow=n,ncol=n,byrow=TRUE))*A0) # neighbor treatment level
    # construct neighborhood covariates, the average of neighbors
    k <- ncol(covar) # num of covariates
    covnei <- list()
     for i in (1:k) {
      covnei[[i]] <- rowSums(matrix(covar[,i],nrow=n,ncol=n,byrow=TRUE))*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
  } else if (Atype=="adj") {
    # define treatment levels tl
    A0 <- A
    A0 <- A0/rowSums(A0) # row std 
    trnei <- rowSums(matrix(tr,nrow=n,ncol=n,byrow=TRUE))*A0)
    # construct neighborhood covariates, first order neighborhood means and network properties
    k <- ncol(covar)
    covnei <- list()
    for i in (1:k) {
      covnei[[i]] <- rowSums(matrix(covar[,i],nrow=n,ncol=n,byrow=TRUE))*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
    netA <- igraph::graph_from_adjacency_matrix(A,mode="undirected",weighted=FALSE)
    dgr <- rowSums(A) # the node degrees
    btwnss <- betweenness(netA, weights = NA) # betweeness centrality
    covnei <- cbind.data.frame(covnei,dgr,btwnss)
  }

  # fit treatment model and estimate propensity scores
  if (pred_model=="lm"){
    # p(Z|X_z,G)
    T_1<-lm(x=c(covar,trnei),y=tr)
    T_1_beta<-coef(T_1)
    T_1_Xb<-model.matrix(T_1)%*%T_1_beta
    T_1_sigma <- summary(T_1)$sigma
    gps_z<-dnorm(tr, mean=T_1_Xb, sd=T_1_sigma)
    # p(G|X_g)
    T_2<-lm(x=covnei,y=trnei)
    T_2_beta<-coef(T_2)
    T_2_Xb<-model.matrix(T_2)%*%T_2_beta
    T_2_sigma <- summary(T_2)$sigma
    gps_g<-dnorm(trnei, mean=T_2_Xb, sd=T_2_sigma) 
    gps_j<-gps_z*gps_g 
  }
  ps<-cbind.data.frame(gps_j,gps_z,gps_g)
  colnames(ps)<-c("t11","t10","t01","t00","z","g")
  return(ps)
}
```



## Introduce GPS to weighted conformal prediction to obtain counterfactuals. 

The generalised propensity restores the ignorability and SUTVA assumptions underpinning classic PO framework. In the case of non-trivial spatial interference, we try to estimate the total effect $Y_{i}(Z_{i}=1,G_{-i}=1)-Y_{i}(Z_{i}=0,G_{-i}=0)$ , direct effect $Y_{i}(Z_{i}=1,G_{-i}=0)-Y_{i}(Z_{i}=0,G_{-i}=0)$ and spillover effect $Y_{i}(Z_{i}=0,G_{-i}=1)-Y_{i}(Z_{i}=0,G_{-i}=0)$.

ref: 
Lei (2021)
https://github.com/lihualei71/cfcausal

Tibshirani(2019)
https://github.com/ryantibs/conformal/tree/master/tibshirani2019
https://github.com/ryantibs/conformal/tree/master/conformalInference

Candes(2022)
https://rinafb.github.io/code/nonexchangeable_conformal.zip.

standard conformal inference is based on conditional mean estimation.

```{r utils, include=FALSE, eval=FALSE}
source("utils.R", local = knitr::knit_global())
#  imports conformalScore, gen_cv_ids, weightedConformalCutoff, RF, quantRF, Boosting
```


### generic weighted split conformal (adapted form Tibshirani, 2019)
splits data into training and val sets. The weighting across training and test sets provides robustness against distribution shift. The weighting can be exploited to make counterfactual predictions, as Lei et.al. (2021) has demonstarted.


```{r conformalSplit}
# conformalSplit() is the base prediction function, a weighted split cf.

# to do 1: fix function to take a testid list

# X, Y
# Xtest     X of testing point. dataframe 1-m rows
# wt_test   wt of testing point. dataframe 1-m rows
# outfun    function for predicting Y
# CQR       whether use quantile regression for Ymodel
# wt        weights of all units 
# alpha     target error rate
# trainid   list/ vector

# output prediction intervals
# cf type="mean", CI two sided


conformalSplit <- function(X, Y, Xtest,wt_test,
                          outfun, CQR=FALSE,
                          wt,
                          trainid,
                          alpha=0.1) {
  # subset training data and val data
  Xtrain <- X[trainid, ,drop=FALSE]
  Ytrain <- Y[trainid]
  Xcal <- X[-trainid, ,drop=FALSE]
  Ycal <- Y[-trainid]
  
  censoring <- function(x){pmin(pmax(x, 0.05), 20)}
  m <- length(Xtest) # the number of testing points
  qt <- c() # vector qt for each test point.
  for (i in 1:m) {
       wt_cal <- wt[-trainid]
       avg_wt <- mean(c(wt_cal, wt_test[i])) # wt_test is a vector
       wt_cal <- censoring(wt_cal/avg_wt)
       wt_test[i] <- censoring(wt_test[i]/avg_wt)
       totw <- sum(wt_cal)
       #wt_cal <- wt_cal / totw
       qt[i] <- (1 + wt_test[i] / totw) * (1 - alpha) # quantile 1-alpha weighted
       qt[i] <- pmin(qt[i], 1)
  }
  
  if (CQR==FALSE){
    # mean prediction, default outfun=RF
    #Ymodel <- function(x){do.call(outfun, list(Y = Ytrain, X = Xtrain,Xtest=x))}
    Ymodel <- randomForest::randomForest(x = Xtrain, y = Ytrain)
    Ycal_hat <- predict(Ymodel, newdata = Xcal)
    Yscore <- conformalScore(Ycal, Ycal_hat,quantiles=0)
    
    Ytest_hat <- predict(Ymodel, newdata = Xtest) # numeric vector
    
    Yslack <- weightedConformalCutoff(Yscore, wt_cal, qt) 
    Ylo <- Ytest_hat - Yslack
    Yup <- Ytest_hat + Yslack
    out <- data.frame(lower = Ylo, upper = Yup)
  } else{
    # conformalised quantile regression. default outfun= QRF
    Ymodel <- grf::quantile_forest(X = Xtrain, Y = Ytrain,quantiles = c(0.05,0.95))
    Ycal_hat <- predict(Ymodel, Xcal,quantiles = c(0.05,0.95))
    Ycal_hat <- Ycal_hat$predictions
    Yscore <- conformalScore(Ycal, Ycal_hat,quantiles=1)
    
    Ytest_hat <- predict(Ymodel, Xtest,quantiles = c(0.05,0.95))
    Ytest_hat <-Ytest_hat$predictions # matrix
    
    Yslack <- weightedConformalCutoff(Yscore, wt_cal, qt) 
    Ylo <- Ytest_hat[,1] - Yslack
    Yup <- Ytest_hat[,2] + Yslack
    out <- data.frame(lower = Ylo, upper = Yup)
  }
  return(out)
}

```


### counterfactual prediction with ps weighted split conformal, binary treatment (adapted from Lei, 2021)

Counterfactual conformal prediction is an extension of generic conformal prediction. It is an application of weighted confomal prediction.  The main feature of counterfactual conformal prediction is the introduction of propensity scores e(x) for the weighting. The ps weighted conformal inference is in spirit similar to IPW. In the IPW estimators, the weights calibrate the observed covariate distribution to the target one.  Similarly, in counterfactual conformal prediction, the weighting addresses the covariate distribution shift between treatment and control groups (i.e. the sampling and target population when fitting model on treatment group observations and make counterfactual prediction on control group individuals).

For counterfactual inference on Y(1), use 1/e(x) as weights; for Y(0), use 1/(1-e(x)).

The estimation procedure has lovely doubly robust properties.

```{r conformalCf_split}
# conformalCf_split() formats data and params, pass them to conformalSplit() to predict counterfactuals

# to do: take quantiles; take test id list

# X, Y        observed X and Y of all units
# Xtest       data point to do prediction on, single point 
# Ytest       observed Y value for test point, single point 
# tltest      observed exposure level of test point, single point 
# wt_test     Propensity of test point receiving observed treatment, single point 
# tl          vector of exposure levels to all units
# A           spatial network A_ji, in the form of adjacency matrix or distance matrix 
# Atype       type of A, "dist" or "adj", 
# ps_pred_model passed to conformalSplit()
# outfun      passed to conformalSplit()
# CQR         passed to conformalSplit()
# ps          passed to conformalSplit()
# trainid     optional. interface to other sampling methods. Internal default is random sampling

conformalCf_split <- function(X, Y, 
                              Xtest,Ytest,tltest,wt_test,
                              tl,
                              outfun, CQR=FALSE,
                              ps,trainid){
  
  inds11 <- which(tl == "t11")
  inds10 <- which(tl == "t10")
  inds01 <- which(tl == "t01")
  inds00 <- which(tl == "t00")
  inds<-list(inds11,inds10,inds01,inds00) # list of index, careful with the ordering
 
   # parse index
  X0<-Y0<-ps0<-out0<-trainid0<-list()
  n0<-lengths(inds)
  minn<-min(n0)
  testn<-floor(minn*0.6)
  
  for (i in 1:4){
    if (is.na(trainid)){
      trainid0[[i]]<- sample(n0[[i]], testn) # same size samples
    } else {trainid0[[i]]<-trainid[[i]]}
    X0[[i]] <- X[inds[[i]], ,drop=FALSE]
    Y0[[i]] <- Y[inds[[i]]]
    ps0[[i]]<- ps[inds[[i]], ,drop=FALSE] 
    # X0, Y0 has 4 elements, each specific to t11 t10 t01 t00
    # trainid0 each element is train id for each of the X0 Y0 elements.
    # ps0 has 6 cols, 11, 10, 01, 00, (z, g)
  } 
  
  
  # parse the test points into four groups by tl types.
  # make predictions for each tl type test points and combine the results.
  test_grp11<-which(tltest=="t11")# return index within the test points
  if ( length(test_grp11)!=0){
      Xtest_grp11<-Xtest[test_grp11, ,drop=FALSE]
      Ytest_grp11<-Ytest[test_grp11]
      wttest_grp11<-wt_test[test_grp11]
      
      out1 <- data.frame(lower=Ytest_grp11,upper=Ytest_grp11)
      
      out2 <- conformalSplit(X0[[2]], Y0[[2]],Xtest_grp11,wttest_grp11,outfun,CQR,
                           wt=ps0[[2]][,1]/ps0[[2]][,2],trainid0[[2]])
    
      out3 <- conformalSplit(X0[[3]], Y0[[3]],Xtest_grp11,wttest_grp11,outfun,CQR,
                           wt=ps0[[3]][,1]/ps0[[3]][,3],trainid0[[3]])
    
      out4 <- conformalSplit(X0[[4]], Y0[[4]],Xtest_grp11,wttest_grp11,outfun,CQR,
                           wt=ps0[[4]][,1]/ps0[[4]][,4],trainid0[[4]])
      
      out11<- cbind.data.frame(out1,out2,out3,out4)
      
  } else {
    out11<- data.frame()
    message ("No test ponits with tl=t11")}
  
  test_grp10<-which(tltest=="t10") 
  if ( length(test_grp10)!=0) {
      Xtest_grp10 <- Xtest[test_grp10, ,drop=FALSE]
      Ytest_grp10 <- Ytest[test_grp10]
      wttest_grp10 <- wt_test[test_grp10]
      
      out1 <- conformalSplit(X0[[1]], Y0[[1]], Xtest_grp10,wttest_grp10,outfun,CQR,
                           wt=ps0[[1]][,2]/ps0[[1]][,1],trainid0[[1]])
      
      out2 <- data.frame(lower=Ytest_grp10,upper=Ytest_grp10)
    
      out3 <- conformalSplit(X0[[3]], Y0[[3]],  Xtest_grp10,wttest_grp10,outfun,CQR,
                           wt=ps0[[3]][,2]/ps0[[3]][,3],trainid0[[3]])
    
      out4 <- conformalSplit(X0[[4]], Y0[[4]],  Xtest_grp10,wttest_grp10,outfun,CQR,
                           wt=ps0[[4]][,2]/ps0[[4]][,4],trainid0[[4]])
      
      out10<- cbind.data.frame(out1,out2,out3,out4)
  } else {
    out10<- data.frame()
    message ("No test ponits with tl=t10")}
    
  test_grp01 <- which(tltest=="t01") 
  if ( length(test_grp01)!=0) {
      Xtest_grp01 <- Xtest[test_grp01, ,drop=FALSE]
      Ytest_grp01 <- Ytest[test_grp01]
      wttest_grp01 <- wt_test[test_grp01]
      out1 <- conformalSplit(X0[[1]], Y0[[1]],Xtest_grp01,wttest_grp01,outfun,CQR,
                           wt=ps0[[1]][,3]/ps0[[1]][,1],trainid0[[1]])
    
      out2 <- conformalSplit(X0[[2]], Y0[[2]], Xtest_grp01,wttest_grp01,outfun,CQR,
                           wt=ps0[[2]][,3]/ps0[[2]][,2],trainid0[[2]])
      
      out3 <- data.frame(lower=Ytest_grp01,upper=Ytest_grp01)
    
      out4 <- conformalSplit(X0[[4]], Y0[[4]], Xtest_grp01,wttest_grp01,outfun,CQR,
                           wt=ps0[[4]][,3]/ps0[[4]][,4],trainid0[[4]])
      
      out01<- cbind.data.frame(out1,out2,out3,out4)
      
  } else {
    out01<- data.frame()
    message ("No test ponits with tl=t01")}
  
  test_grp00 <- which(tltest=="t00") # checked, correct, numeric vector (integer)
  if ( length(test_grp00)!=0) {
      Xtest_grp00 <- Xtest[test_grp00, ,drop=FALSE]
      Ytest_grp00 <- Ytest[test_grp00]
      wttest_grp00 <- wt_test[test_grp00]
      out1 <- conformalSplit(X0[[1]], Y0[[1]], Xtest_grp00,wttest_grp00,outfun,CQR,
                           wt=ps0[[1]][,4]/ps0[[1]][,1],trainid0[[1]])
    
      out2 <- conformalSplit(X0[[2]], Y0[[2]], Xtest_grp00,wttest_grp00,outfun,CQR,
                           wt=ps0[[2]][,4]/ps0[[2]][,2],trainid0[[2]])
    
      out3 <- conformalSplit(X0[[3]], Y0[[3]], Xtest_grp00,wttest_grp00,outfun,CQR,
                           wt=ps0[[3]][,4]/ps0[[3]][,3],trainid0[[3]])
      out4 <- data.frame(lower=Ytest_grp00,upper=Ytest_grp00)
      
      out00<- cbind.data.frame(out1,out2,out3,out4) 
      
  } else {
    out00<- data.frame()
    message ("No test ponits with tl=t00")}
  
  out <- rbind.data.frame(out11,out10,out01,out00)
  out <- na.omit(out)
  # outcome has 8 cols, being the lower and upper bounds of the 4 POs. 
  # outcome has same m of rows as testid length
  return(out)
}

```


### ITE prediction with ps weighted split conformal. Under spatial interference, binary treatment. (new)

Here we apply our causal effects estimands (with bianry treatment, four levels of exposure) and associated gps weights. 

Should be careful what average ITE means though. The neighborhood treatment level (i.e. the local spatial interference pattern) always depends on where the ego unit is located in the spatial network, how informative is averaging over it?


```{r ITE_version1}
# conformal_ITE1() is the outer func that constructs ITE from Y_obs and counterfactuals

# X, Y
# testid      index of Xtest point in full X. a single index or a numerical vector length m.
# tr          vector of individual binary treatments, numeric.
# A           spatial network A_ji, in the form of adjacency matrix or distance matrix. 
# Atype       type of A, "dist" or "adj".
# ps_pred_model 
# outfun      passed to conformalSplit()
# CQR         passed to conformalSplit()
# tranid      optional. list of length 4. trainid for each treatment level.
# retrainYmodel If true, Ymodel retrained for each test point. If false, Ymodel fitted only once to make predictions for all points.

# output      ITEs and POs

conformal_ITE1 <- function(X, Y, testid,
                          tr,
                          A, Atype,
                          ps_pred_model="binomial",
                          outfun, CQR=FALSE,trainid=NA,
                          retrainYmodel=TRUE
                          ){
  # use psfun="get_gps1"
  PO<-list()
  ITE<-list()
  n <- length(Y)
  m <- length(testid)
 
  # compute joint and marginal propensity scores
  # ps are used here to calculate ITEs, and passed on to conformalSplit() as weights
  ps<-get_gps1(tr,covar=X,A,Atype,pstype="joint",ps_pred_model="binomial") # dataframe n*6
  
  # compute counterfactuals
  trnei<-get_trnei1(tr,A,Atype)
  tl <- paste0("t",tr,trnei) # tls ("t00","t01","t10","t11")
  
  if (retrainYmodel==TRUE) {
     for (i in 1:m){
        Ztest <- tr[testid[i]] 
        Gtest <- trnei[testid[i]] 
        tltest <- paste0("t",Ztest,Gtest) 
        if(tltest=="t11"){wt_test<- ps[testid[i],1]
        } else if (tltest=="t10") {wt_test<- ps[testid[i],2]
        } else if (tltest=="t01") {wt_test<- ps[testid[i],3]
        } else {wt_test<- ps[testid[i],4]
     }
     Xtest <- X[testid[i], ,drop=FALSE]
     Ytest <- Y[testid[i]]
     if(is.na(trainid)){
         Cf_params<-list(X,Y,Xtest,Ytest,tltest,wt_test,tl,outfun,CQR,ps=ps,trainid=NA)
       } else {  stop("If trainid not provided, please input NA.")
       }
     PO_temp<-do.call(conformalCf_split,Cf_params) # get the four potential outcomes
     # PO is a dataframe of 8 cols: the lo and up bounds of the 4 POs re 11 10 01 00.
     ITE[[i]]<-data.frame(spill1_lo=PO_temp[[1]] - PO_temp[[4]],
                           spill1_up=PO_temp[[2]] - PO_temp[[3]],
                           spill0_lo=PO_temp[[5]] - PO_temp[[8]],
                           spill0_up=PO_temp[[6]] - PO_temp[[7]],
                           direct1_lo=PO_temp[[1]] - PO_temp[[6]],
                           direct1_up=PO_temp[[2]] - PO_temp[[5]],
                           direct0_lo=PO_temp[[3]] - PO_temp[[8]],
                           direct0_up=PO_temp[[4]] - PO_temp[[7]] )
     PO[[i]]<-PO_temp
     }
     ITE <- do.call("rbind", ITE)
       
  } else if (retrainYmodel==FALSE) {
     Ztest <- tr[testid] # vector
     Gtest <- trnei[testid] # vector
     tltest <- paste0("t",Ztest,Gtest)  # vector
     wt_test <- list()
     for (i in 1:m) {
         if(tltest[i]=="t11"){wt_test[i]<- ps[testid[i],1]
         } else if (tltest[i]=="t10") {wt_test[i]<- ps[testid[i],2]
         } else if (tltest[i]=="t01") {wt_test[i]<- ps[testid[i],3]
         } else {wt_test[i]<- ps[testid[i],4]
         }
     } # list wt_test
     Xtest <- X[testid, ,drop=FALSE]
     Ytest <- Y[testid]
     if(is.na(trainid)){
         Cf_params<-list(X,Y,Xtest,Ytest,tltest,wt_test,tl,outfun,CQR,ps=ps,trainid=NA)
       } else { stop("If trainid not provided, please input NA.")
       }
     PO_temp <-do.call(conformalCf_split,Cf_params) 
     # PO is a dataframe of 8 cols: the lo and up bounds of the 4 POs re 11 10 01 00.
     ITE <- data.frame(spill1_lo=PO_temp[[1]] - PO_temp[[4]],
                       spill1_up=PO_temp[[2]] - PO_temp[[3]],
                       spill0_lo=PO_temp[[5]] - PO_temp[[8]],
                       spill0_up=PO_temp[[6]] - PO_temp[[7]],
                       direct1_lo=PO_temp[[1]] - PO_temp[[6]],
                       direct1_up=PO_temp[[2]] - PO_temp[[5]],
                       direct0_lo=PO_temp[[3]] - PO_temp[[8]],
                       direct0_up=PO_temp[[4]] - PO_temp[[7]] )
     PO[[i]]<-PO_temp
  }
  
  PO <- do.call(rbind,PO)
  return (list(PO,ITE))
}
```


### ITE prediction with ps weighted split conformal. Under spatial interference, continuous treatment(s). (new)

More generally, the procedure can be extended to continuous exposure levels. To implement, neighborhood treatment levels [0,1] are discretized to intervals of 0.01. counterfactuals are predicted for each level. 

This version takes longer to run since the target is dose-response curve. Rationale for using this may be robustness against distribution shift and non-stationarity. But it is unclear how significant the advantage is. Parametric models may actually be a better option for continuous treatments.

```{r ITE_version2}
# conformal_ITE2() constructs ITE / dose-response curves from observed outcome and predicted

```




## Refinement1: non-exchangeabilitya and adjustments. (new)

Doable: To handle non-exchangeability in spatial datasets, higher weights are given to local samples, ref Candes 2022. Not exactly doable: Chernozhukov 2018, time series data points are divided into blocks, in each block, an autoregressive model is fitted to estimate Y and obtain residuals; the residual vector then produce a conformity score. The block slides through the times series (the data permutes by cyclic sliding).

```{r get_w_ids}
# creates train id for conformalSplit()

# for given ocation s0, the nearest m support points (by some distance measure, most commonly geo distance) are given higher weights to base the conformal prediction on
# this dist info is already supplied in the A matrix we used for obtaining gps
# the m points can be equally weighted or given a dist decay

```


```{python CP_LS}
# ref Candes 2022 
# conformal prediction with weighted least squares

# the func seems to be for a single test point x, for which the weights w(x,Z_tr) are supplied. Scale it to a list of X and their weights?
def CP_LS(X,Y,x,alpha,weights=[],tags=[]):
    # weights w(X_i,Z_tr) are used for computing quantiles for the prediction interval 
    # tags are used as weights in weighted least squares regression
    n = len(Y)
    
    if(len(tags)==0):
        tags = np.ones(n+1)
    if(len(weights)==0):
        weights = np.ones(n+1)
    if(len(weights)==n):
        weights = np.r_[weights,1]
    weights = weights / np.sum(weights)
    # randomly permute one weight for the regression
    random_ind = int(np.where(np.random.multinomial(1,weights,1))[1])
    tags[np.c_[random_ind,n]] = tags[np.c_[n,random_ind]]
    # least squares estimation of Y
    XtX = (X.T*tags[:-1]).dot(X) + np.outer(x,x)*tags[-1]
    a = Y - X.dot(np.linalg.solve(XtX,(X.T*tags[:-1]).dot(Y)))
    b = -X.dot(np.linalg.solve(XtX,x))*tags[-1]
    a1 = -x.T.dot(np.linalg.solve(XtX,(X.T*tags[:-1]).dot(Y)))
    b1 = 1 - x.T.dot(np.linalg.solve(XtX,x))*tags[-1]
    # if we run weighted least squares on (X[1,],Y[1]),...(X[n,],Y[n]),(x,y)
    # then a + b*y = residuals of data points 1,..,n
    # and a1 + b1*y = residual of data point n+1
    
    y_knots = np.sort(np.unique(np.r_[((a-a1)/(b1-b)),((-a-a1)/(b1+b))]))
    y_inds_keep = np.where( ((np.abs(np.outer(a1+b1*y_knots,np.ones(n))) > \
       np.abs(np.outer(np.ones(len(y_knots)),a)+np.outer(y_knots,b))) *\
                             weights[:-1] ).sum(1) <= 1-alpha )[0] 
    # prediction interval
    y_PI = np.array([y_knots[y_inds_keep.min()],y_knots[y_inds_keep.max()]])
    
    return y_PI
```



## Refinement2: Cross validation 

### generic weighted conformal with CV (adapted from Tibshirani, 2019; Lei, 2021)
In the following 2 code chunks, for each of the folds, the generic split weighted conformal procedure defined earlier is carried out. Computation wise, using CV is more efficient than the traditional split conformal. 
```{r get_cv_ids}
# Generate a list of indices for cross-validation
get_cv_ids <- function(n, nfolds, offset = 0){
    ids <- sample(n, n)
    quo <- floor(n / nfolds)
    if (quo == 0){
        idlist <- lapply(1:nfolds, function(i){
            if (i <= n){
                i
            } else {
                numeric(0)
            }
        })
    } else {
        resid <- n - quo * nfolds
        idlist <- lapply(1:nfolds, function(i){
            tmp <- (i - 1) * quo + 1:quo
            if (i <= resid){
                tmp <- c(tmp, quo * nfolds + i)
            }
            return(ids[tmp] + offset)
        })
    }
    return(idlist)
}
```


```{r conformalCV}
# conformalCV() is a variation from conformalSplit() , adding CV

# X, Y
# outfun
# outparams (if CQR quantiles should be part of outparams, not necessary for mean estimation)
# wtfun     a list of functions, length nfolds
# alpha
# nfolds
# idlist    needs to a list of trainid of length 'nfolds'
# Xtest testing covariates.
# output prediction interval

conformalCV <- function(X, Y, Xtest,
                        outfun, outparams,
                        wt,
                        alpha=0.1,
                        nfolds, idlist){
  outparams0 <- outparams
  
  # generate trainid list
  n <- length(Y)
  if (is.null(idlist)){idlist <- gen_cv_ids(n, nfolds)}  
  
  temp <- list()
  for (k in 1:nfolds){

      testid <- idlist[[k]]
      Xtrain <- X[-testid, ,drop=FALSE]
      Ytrain <- Y[-testid]
      Xval <- X[testid, ,drop=FALSE]
      Yval <- Y[testid]

      outparams <- c(list(Y = Ytrain, X = Xtrain), outparams0)
      Ymodel <- function(X){ do.call(outfun, c(outparams, list(Xtest = X)))}

      Yhat <- Ymodel(Xval)
      Yscore <- conformalScore(Yval, Yhat)
      wt <- wt[[k]][-testid]

      Yhat_test <- Ymodel(Xtest)
      
      wt_test <- wtfun[[k]](Xtest)

      avg_wt <- mean(c(wt, wt_test))
      wthigh <- 20 
      wtlow <- 0.05
      wt <- censoring(wt/avg_wt, wthigh, wtlow)
      wt_test <- censoring(wt_test/avg_wt, wthigh, wtlow)
      totw <- sum(wt)
      wt <- wt / totw

      qt <- (1 + wt_test / totw) * (1 - alpha)
      qt <- pmin(qt, 1) 
      Yslack <- weightedConformalCutoff(Yscore, wt, qt) 

      Ylo <- Yhat_test - Yslack
      Yup <- Yhat_test + Yslack

      temp[[k]] <- data.frame(lower = Ylo, upper = Yup)
    
  }
  
  out <- do.call("rbind",temp) # not exactly right
  
  return(out)
}
  
```

### generic weighted conformal with k-fold spatial CV (new)

Is spatial CV necessary?

spatial CV methods are typically motivated by a few concerns. For example: (1) correlation between training and validation sets (e.g. through spatial overlap) undermines validation set's ability to signal overfitting. (2) implicit spatial info hinders model's transferability to other spatial domains. (3) folds-splitting that does not preserve spatial structures (esp. random resampling that ignores spatial clusters) could lead to biased model. A variety of spatial CV techniques for model training and assessment are at our disposal depending on the priorities.

In the conformal inference literature, thoughts on spatial CV are scarce. In our study, when the goal is counterfactual prediction for the sampling population, we are much less concerned about overfitting or transferability than model bias. Therefore, if necessary, we use CV with spatial blocking. 

gen_spacv_ids() defined in the following code chunk can replace gen_cv_ids() to implement spatial CV in conformal procedures. 

The samples within each CV blocks may or may not satisfy approximate exchangeability. Unless they do, spatial CV alone does not solve the problem of non-exchangeability within each fold. For that, we may still need the local resampling trick (or something equivalent).


```{r get_spacv_ids}
# Generate a list of indices for spatial cross validation
# modified version of gen_cv_ids()

# takes distance matrix/ sparse adjacency matrix A as input
# spectral clustering on the matrix to create spatial folds

# alternatively, can input sf object and create grided folds with blockCV::spatialBlock

gen_spacv_ids <- function(n,nfolds, A, Atype="adj"){
  idlist <- list()
  idx<-rep(1,n)
  if (Atype=="adj") {
    # netA <- igraph::graph_from_adjacency_matrix(A,mode="undirected",weighted=FALSE)
    # clu <- cluster_walktrap(netA) 
    clu <- spectral(A,nfolds) # return vec of group membership
  } else if (Atype=="dist") {
    # clu <- adjclust::adjClust(A,type="dissimilarity",h=1000) # returns a tree
    # dependency on sparseMatrixStats? requires sparse matrix?
    clu <- hclust(A)
    clu <- cutree(clu,k=nfolds) # return vec of group membership
  }
  
  if (is.na(clu)){stop('Failed clustering.')}
    
  t <- cbind(idx,clu)
  for (i in 1:nfolds) {idlist[[i]]<-t$idx[which(t$clu==i),]}
  
  # need to regularize sizes of clusters!!!
  # check list sizes
  print(paste("The number of samples in each fold:",lengths(idlist)))
  
  return(idlist)
}
```


Refinement 1 and refinement 2 can be incorporated into the GPS conformal ITE prediction procedure.

