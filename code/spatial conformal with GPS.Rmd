---
title: "sp conformal with gps"
author: "mira"
date: "2022/6/12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(ggplot2)
library(viridis)
library(argparse)
library(grf)
library(gbm)
library(randomForest)
library(bartMachine)
library(igraph)
#library(SuperLearner)
library(xgboost)
library(MASS)
library(nnet)
#library(causalToolbox)

```

# Intro
Why spatial interference.

Why ITE in favor of ATE CATE CQTE: Better account for treatment effect heterogeneity (which could result from spatial process).

Why intervals instead of point estimate: Uncertainty quantification for sensitive decision making. Better coverage.

Why conformal prediction: Non-parametric. Flexible. Accommodates various prediction models.

How to handle non-exchangeability in spatial conformal prediction: Assume ‘local’ exchangeability, using distance kernel to restrict scope of training data points. Or use global spatial CV technique.

How to handle spatial interference: In the case of non-trivial spatial interference, treatment effect defined as total effect, direct and spillover effect. For estimation, introduce generalised propensity score, which defines treatment probability as a function of ego and neighbouring units’ covariates. The generalised propensity restores the ignorability and SUTVA assumptions underpinning classic PO framework. Besides individual level covariates, network and neighbourhood properties (such as centrality metrics or local averages) can also be included (Forastiere et.al., 2021). Network uncertainty can also be incorporated (may or may not have time for this in this project).

Relation to prior work: Extension and synthesis of prior work to spatial interference (Forastiere), bringing together conformal counterfactual prediction (Barber, Candes, Lei) and generalised propensity score (Hirano & Imbens) based techniques.



# The proposed routine of analysis
## Get generalised propensity score for spatial interference

With interference, there can be multiple levels of exposure to unit $i$ even for binary treatments. That is, the actual exposure level is a function of the unit's treatment $Z_{i}$ and others' treatment program $G_{-i}$. The generalised propensity score $e(x,g)$ (Hirano & Imbens, 2004) has been developed for these scenarios (Forastiere et.al., 2021). Exposure level can be treated as either continuous or discretised. Here, for simplicity, we start with four levels of treatment,  $(Z_{i}=0,G_{-i}=0)$ , $(Z_{i}=0,G_{-i}=1)$ , $(Z_{i}=1,G_{-i}=0)$ , $(Z_{i}=1,G_{-i}=1)$. Treatment $G_{-i}$ is defined as a function of the proportion of unit i's neighbours getting treatment. The covariates to predict the propensity score can include individual level attributes of unit $i$, as well as neighbourhood level aggregate attributes and spatial network structural properties (such as network centrality metrics).

ref: CausalGPS::estimate_gps
https://github.com/cran/CausalGPS

```{r get_gps, include=TRUE, eval=FALSE}
# the psfun
# input
# tr          col vector of individual level binary treatments
# covar       dataframe of covariates
# A           spatial network, adjacency matrix or distance matrix 
# Atype       type of A, "dist" or "adj", A does not have to be symmetric
# pstype      two forms of ps: "joint" e(x)=P(Z=1,G=g|X=x), "conditional" e(x,g)=P(Z=1|G=g,X=x)
# pred_model  "xgboost" or "multinom" or "binom" (binom for conditional ps)
# output
# ps          vector of propensity score

get_gps <- function(tr,
                    covar,
                    A,
                    Atype="dist",
                    pstype="joint",
                    pred_model="multinom"){
  # check args dimensions and types
  
  
  # format treatment and covariates
  n<-length(tr)
  
  if Atype="dist" {
    # define treatment levels tl based on tr and A, stratified to 4 treatment levels
    A0 <- A
    diag(A0) <-1
    A0 <- A^(-1)
    diag(A0) <- 0
    A0 <- A0/rowSums(A0) # row std inverse dist, can try other dist decay func
    trnei <- rowSums(replicate(n,tr)*A0) # neighbor treatment level [0,1]
    trnei <- ifelse(trnei>0.5,1,0) # neighbor treatment {0,1}, 0.5 is arbitrary here
    tl <- paste0(tr,trnei) # treatment level, multinomial
    # alternatively tl can be ordinal/continuous, can be a vec of length 2
    
    # construct neighborhood covariates, the average of neighbors
    k <- ncol(covar) # num of covariates
    for i in (1:k) {
      covnei[[i]] <- rowSums(replicate(n,covar[,i])*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
    covar0 <- cbind(covar,covnei)
  }
  
  if Atype="adj" {
    # the adjacency matrix can be simply geo contiguity or otherwise defined
    # assume A is unweighted (if directed, j should be in the row names)
    # define treatment levels tl
    A0 <- A
    A0 <- A0/rowSums(A0) # row std 
    trnei <- rowSums(replicate(n,tr)*A0)
    trnei <- ifelse(trnei>0.5,1,0)
    tl <- paste0(tr,trnei)
    
    # construct neighborhood covariates, neighborhood means and network properties
    k <- ncol(covar)
    for i in (1:k) {
      covnei[[i]] <- rowSums(replicate(n,covar[,i])*A0)
    }
    covnei <- as.data.frame(do.call(cbind,covnei))
    netA <- igraph::graph_from_adjacency_matrix(A,mode="undirected",weighted=FALSE)
    btwnss <- betweenness(netA, weights = NA) # can try other metrics
    covar0 <- cbind(covar,covnei,btwnss)
  }
 
   
  # fit the treatment model and estimate the propensity scores
  if pstype="joint"{
    if pred_model="multinom"{
         pred0 <- nnet::multinom(x=covar0,y=tl)

    }
    if pred_model="xgboost"{
         pred0 <- SL.xgboost(Y=tl,X=covar0,family="multinomial")

    }
    if pred_model="binom"{stop('Joint ps cannot be estimated with binomial model.')}
  }
  
  if pstype="conditional"{
     # by default use pred_model=="binom"
    if pred_model="xgboost"{
         pred0 <- SL.xgboost(Y=tr,X=cbind.data.frame(covar0,trnei),family="binomial")
    }
    else{
         pred0 <- plm.fit(x=cbind.data.frame(covar0,trnei),y=tr,family="binomial")
         sd <- pred0$sigma 
         resi <- residuals(pred0)
         ps <- dnorm(resi,0,sd) 
    }    
  }
  
  # return the estimated propensity score
  return(ps)
}
```


## Introduce GPS to weighted conformal prediction to obtain counterfactuals. 

The generalised propensity restores the ignorability and SUTVA assumptions underpinning classic PO framework. In the case of non-trivial spatial interference, we try to estimate the total effect $Y_{i}(Z_{i}=1,G_{-i}=1)-Y_{i}(Z_{i}=0,G_{-i}=0)$ , direct effect $Y_{i}(Z_{i}=1,G_{-i}=0)-Y_{i}(Z_{i}=0,G_{-i}=0)$ and spillover effect $Y_{i}(Z_{i}=0,G_{-i}=1)-Y_{i}(Z_{i}=0,G_{-i}=0)$.

ref: 
Lei, L. & Candes, E. (2021) Conformal Inference of Counterfactuals and Individual Treatment Effects. arXiv:2006.06138v2 


```{r utils, eval=FALSE}
source("utils.R")
#  imports conformalScore, gen_cv_ids, weightedConformalCutoff, RF, quantRF, Boosting
```


```{r conformal_CV, include=TRUE, eval=FALSE}
# conformalCV 
# type ==CQR, both sides, quantiles=(0.05, 0.95)

# params
# X testing set 
# Y
# outfun
# outparams list of other params for the outfun, e.g. see {\quantRF}
# wtfun weight function
# nfolds
# idlist 
# Xtest testing covariates.
# alpha confidence level.
# wthigh upper truncation
# wtlow lower truncation. extreme weights are truncated to ensure stability.

# return predictive intervals. n*2 data.frame 

conformalCV <- function(X, Y, outfun, wtfun, nfolds,idlist,
                        Xtest,alpha = 0.1,wthigh = 20, wtlow = 0.05){
   
   wtfun0 <- NULL
    if (is.null(wtfun)){
        wtfun <- lapply(1:nfolds, function(k){function(X){rep(1, nrow(X))}})
    } else if (is.function(wtfun)){
        wtfun0 <- wtfun
        wtfun <- rep(list(wtfun), nfolds)
    } 
    if (is.null(wtfun0)){wtfun0 <- wtfun}
    
    #outparams <- c(outparams, list(quantiles = c(0.05, 0.95)))
    outparams <- c(list(quantiles = c(0.05, 0.95)))
    outparams0 <- outparams
    # n <- length(Y)
    # if(is.null(idlist)) {idlist <- gen_cv_ids(n, nfolds)}

    temp <- list()
    for (k in 1:nfolds){

        testid <- idlist[[k]][-251]# drop empty element. testid has NA at the 251 th ?
        Xtrain <- X[-testid, ,drop=FALSE]
        Ytrain <- Y[-testid]
        Xval <- X[testid, ,drop=FALSE] # subscript out of bounds ?
        Yval <- Y[testid]

        outparams <- c(list(Y = Ytrain, X = Xtrain), outparams0)
        
        Ymodel <- function(X){do.call(outfun, c(outparams, list(Xtest = X)))}
        Yhat <- Ymodel(Xval)
        Yhat_test <- Ymodel(Xtest)
        Yscore <- conformalScore(Yval, Yhat)
       
        wt <- wtfun[[k]](Xval)

        temp[[k]] <- list(Yscore = Yscore,
                    wt = wt,
                    Ymodel = Ymodel,
                    Yhat_test = Yhat_test)
    }
                                
    # CV prediction
    censoring <- function(x, high = 20, low = 0.05){pmin(pmax(x, low), high)}
    
    wt <- do.call(c, lapply(temp, function(x){x$wt}))
    wt_test <- wtfun0(Xtest) # wtfun is a list, wtfun0 is a func
    avg_wt <- mean(c(wt, wt_test))
    wt <- censoring(wt / avg_wt, wthigh, wtlow)
    wt_test <- censoring(wt_test / avg_wt, wthigh, wtlow)

    totw <- sum(wt)
    wt <- wt / totw
    qt <- (1 + wt_test / totw) * (1 - alpha)
    qt <- pmin(qt, 1)
    
    # the confidence intervals
    CI <- sapply(1:length(qt), function(i){
        Ylo <- lapply(temp, function(x){x$Yhat_test[i, 1] - x$Yscore})
        Ylo <- do.call(c, Ylo)
        Ylo <- -weightedConformalCutoff(-Ylo, wt, qt[i])
        Yup <- lapply(temp, function(x){x$Yhat_test[i, 2] + x$Yscore})
        Yup <- do.call(c, Yup)
        Yup <- weightedConformalCutoff(Yup, wt, qt[i])
        c(Ylo, Yup)
    })
    res <- data.frame(lower = as.numeric(CI[1, ]),upper = as.numeric(CI[2, ]))
    
    return(res)
}
```


```{r conformalCf_CV, include=TRUE, eval=FALSE}
# estimand == missing, type==CQR, both sides, quantile (0.05, 0.95)

# params:
# X， Y
# outfun
# outparams
# psfun
# psparams
# nfolds
# returns CI (from conformalCV)
conformalCf_CV <- function(X, Y,outfun, psfun,nfolds,Xtest){
  
    T <- as.numeric(!is.na(Y))
    inds1 <- which(T == 1)
    inds0 <- which(T == 0)
    n1 <- length(inds1)
    n0 <- length(inds0)
    
    #nfolds<-4
    idlist1 <- gen_cv_ids(n1, nfolds, offset = 0)
    idlist0 <- gen_cv_ids(n0, nfolds, offset = 0)
    idlist <- lapply(1:nfolds, function(k){
      c(inds1[idlist1[[k]]], inds0[idlist0[[k]]])
    })
    # sum(is.na(idlist))
    
    # psparams0 <- psparams
    
    wtfun <- lapply(1:nfolds, function(k){
        # inverse propensity score as weights
        testid <- idlist[[nfolds]][-251]
        Xtrain <- X[-testid, ,drop=FALSE] 
        Ttrain <- T[-testid]
        #psparams <- c(list(Y = Ttrain, X = Xtrain), psparams0)
        psparams <- c(list(Y = Ttrain, X = Xtrain))
        function(X){
            ps <- do.call(psfun, c(psparams, list(Xtest = X)))
            (1 - ps) / ps
        }
    })
    # psparams <- c(list(Y = T, X = X), psparams0)
    psparams <- c(list(Y = T, X = X))
    wtfun_test <- function(X){
        ps <- do.call(psfun, c(psparams, list(Xtest = Xtest)))
        (1 - ps) / ps }

    X <- X[inds1, ,drop=FALSE]
    Y <- Y[inds1]
    
    # get CI form conformalCV()
    # res <- conformalCV(X, Y,outfun, outparams,wtfun, nfolds, idlist)
    #res$wtfun <- wtfun_test
    res <- conformalCV(X, Y,outfun, wtfun, nfolds, idlist,Xtest)
    
    # return result as a dataframe of prediction intervals
    return(res)
}
```


## Use counterfactual predictions to construct ITE intervals.

If the goal is to obtain ITE for observed samples, since for each individual we already have one potential outcome, the ITEs can be constructed as the differences between the observed outcome and the predicted counterfactual outcomes. It will be more complicated if trying to draw inference on unobserved individuals or even transfer to a new population. This entails predicting all potential outcomes.

```{r in_sample ITE, eval=FALSE}
# One missing potential outcome. 
# CV conformal pred, two sided, quantiles (0.05, 0.95), type==CQR
# params
# X covariates
# Y outcome vector with missing values encoded as NA
# T
# outfun
# outparams
# psfun
# psparams
# nfolds
# Xtest ???
# alpha
# wthigh
# wtlow
# return ITE predction intervals

conformalIte1 <- function(X, Y, T,outfun,psfun, nfolds,
                              Xtest, alpha = 0.1,wthigh = 20, wtlow = 0.05){
    Y1 <- Y0 <- Y
    Y1[T == 0] <- NA
    Y0[T == 1] <- NA
    inds <- which(T == 1)

    # call conformalCf_CV (conformalCV) to make the counterfactual predictions
    # take the difference as ITE estimate
    Y1_CI <- conformalCf_CV(X, Y1,outfun, psfun,nfolds,Xtest)
    Y0_CI <- conformalCf_CV(X, Y0,outfun, psfun,nfolds,Xtest)

    CI <- data.frame(lower = Y1_CI[, 1] - Y0_CI[, 2],upper = Y1_CI[, 2] - Y0_CI[, 1])

    return(CI)
}

```


```{r out_of_sample ITE, include=TRUE, eval=FALSE}
# Naive methods of Conformal inference for individual treatment effects for subjects with both missing potential outcome. 
conformalIte2 <- function(X, Y, T,outfun,psfun, nfolds,
                              Xtest, alpha = 0.1,wthigh = 20, wtlow = 0.05){

}
```


## Further adapt conformal procedure to spatial data 

The basic conformal procedure is not robust to the potential violation of exchangeability in spatial settings. Two possible solutions: (1) based on local exchangeability assumptions. (2) global, spatial CV based. To be realised by wrapping 'conformalCV'. 

```{r local exchangeability, eval=FALSE}



```


```{r , eval=FALSE}

```



